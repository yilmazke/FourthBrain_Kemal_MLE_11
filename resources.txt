https://www.youtube.com/watch?v=EuBBz3bI-aA
https://www.youtube.com/watch?v=PaFPbb66DxQ

https://www.amazon.com/Statistical-Inference-George-Casella/dp/0534243126?language=en_US

https://www.statlearning.com

Sharing an old aha moment. A very old video from good old G.Strang  here https://youtu.be/osh80YCg_GM.
By far the most elegant way to remember the best solution to Ax=b where it is not possible (b is not part of the column space of A) … is simply to solve Ax=p where p is the projection of b on the column space of A… No need to remember the solution (AtAx=At.b so x = (AtA)-1.Atb)

a straight line Linear fit to the log odds…   https://www.youtube.com/watch?v=BfKanl1aSG0

https://arxiv.org/abs/2208.09727

https://www.tylervigen.com/spurious-correlations

https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html
The SGD version includes "learning_rate", which relates to the kind of schedule to use for the learning rate. "eta0" is the parameter that we will use to set the initial learning rate.

https://upload.wikimedia.org/wikipedia/commons/2/26/Precisionrecall.svg

XGBoost
https://xgboost.readthedocs.io/en/stable/

https://rascore.streamlit.app/

https://shap.readthedocs.io/en/latest/genomic_examples.html

https://github.com/slundberg/shap

TPOT: http://epistasislab.github.io/tpot/

https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html

https://langchain.readthedocs.io/en/latest/index.html


Weeok 07 Shared links;
https://arxiv.org/abs/2104.13478 --> Geometric Deep Learning: Grids, Groups, Graphs, Geodesics, and Gauges

https://www.sciencedirect.com/science/article/abs/pii/0893608092900128 --> Kolmogorov's theorem and multilayer neural networks

https://www.sciencedirect.com/science/article/abs/pii/0893608089900038 --> On the approximate realization of continuous mappings by neural networks

https://www.stat.cmu.edu/~cshalizi/mreg/15/lectures/13/lecture-13.pdf --> Lecture 13: Simple Linear Regression in Matrix Format

https://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=4,2&seed=0.26911&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false --> Tensorflow playground

https://bjlkeng.github.io/posts/probabilistic-interpretation-of-regularization/ --> A Probabilistic Interpretation of Regularization

https://www.tensorflow.org/datasets/keras_example --> Training a neural network on MNIST with Keras

https://www.tensorflow.org/tutorials/quickstart/beginner --> Tensorflow exercise for beginners

https://ml.gatech.edu/leadership/polo-chau

https://poloclub.github.io/cnn-explainer/ --> What is a Convolutional neural network

https://poloclub.github.io

https://www.tensorflow.org/tutorials/keras/regression


Week 08 Shared links;
https://towardsdatascience.com/t-distributed-stochastic-neighbor-embedding-t-sne-bb60ff109561

https://www.youtube.com/watch?v=FgakZw6K1QQ --> StatQuest: Principal Component Analysis (PCA), Step-by-Step

https://fouryears.eu/2016/11/23/what-is-the-covariance-matrix/ --> What is the Covariance Matrix?
The covariance matrix is a square matrix that represents the covariance between multiple variables. It is used to measure the linear relationship between pairs of variables in a data set, and the values in the matrix indicate the strength and direction of this relationship. The covariance matrix is a fundamental tool in statistics, machine learning, and other fields where data analysis is required.

how to calculate the eigenvectors and eigenvalues of the covariance matrix
    The eigenvectors and eigenvalues of a covariance matrix can be calculated using the following steps:

        1.Compute the covariance matrix: The covariance matrix is a square matrix that contains the covariance between each pair of variables in the data set.
        2.Find the eigenvalues and eigenvectors of the covariance matrix: To do this, you need to solve the eigenvalue equation Av = λv, where A is the covariance matrix, λ is the eigenvalue, and v is the eigenvector. The eigenvalue equation can be solved using linear algebra techniques, such as the characteristic equation, to find the eigenvalues and eigenvectors.
        3.Normalize the eigenvectors: The eigenvectors should be normalized so that their length is equal to one. This can be done by dividing each eigenvector by its length.
        4.Sort the eigenvectors and eigenvalues: The eigenvectors and eigenvalues should be sorted in descending order based on the magnitude of the eigenvalues. This will ensure that the first eigenvector corresponds to the largest eigenvalue, the second eigenvector corresponds to the second largest eigenvalue, and so on.
    Once you have calculated the eigenvectors and eigenvalues of the covariance matrix, you can use them to perform various data analysis tasks, such as principal component analysis (PCA), dimensionality reduction, and feature extraction.

https://arxiv.org/pdf/2004.05387.pdf --> Vintage Factor Analysis with Varimax Performs Statistical Inference

https://www.robots.ox.ac.uk/~davidc/pubs/tt2015_dac2.pdf --> "The Humble Gaussian Distribution” by David J.C. MacKay 

https://www.youtube.com/watch?v=PFDu9oVAE-g --> Eigenvectors and eigenvalues

https://scikit-learn.org/stable/modules/clustering.html#k-means --> clustering (k-means)

https://www.youtube.com/watch?v=4b5d3muPQmA&ab_channel=StatQuestwithJoshStarmer --> k-means clutering

https://sebastianraschka.com/Articles/2014_python_lda.html#principal-component-analysis-vs-linear-discriminant-analysis --> Principal Component Analysis vs. Linear Discriminant Analysis

https://yangxiaozhou.github.io/assets/2019-10-02/lda_vs_pca.png --> LDA vs PCA