https://www.youtube.com/watch?v=EuBBz3bI-aA
https://www.youtube.com/watch?v=PaFPbb66DxQ

https://www.amazon.com/Statistical-Inference-George-Casella/dp/0534243126?language=en_US

https://www.statlearning.com

Sharing an old aha moment. A very old video from good old G.Strang  here https://youtu.be/osh80YCg_GM.
By far the most elegant way to remember the best solution to Ax=b where it is not possible (b is not part of the column space of A) … is simply to solve Ax=p where p is the projection of b on the column space of A… No need to remember the solution (AtAx=At.b so x = (AtA)-1.Atb)

a straight line Linear fit to the log odds…   https://www.youtube.com/watch?v=BfKanl1aSG0

https://arxiv.org/abs/2208.09727

https://www.tylervigen.com/spurious-correlations

https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html
The SGD version includes "learning_rate", which relates to the kind of schedule to use for the learning rate. "eta0" is the parameter that we will use to set the initial learning rate.

https://upload.wikimedia.org/wikipedia/commons/2/26/Precisionrecall.svg

XGBoost
https://xgboost.readthedocs.io/en/stable/

https://rascore.streamlit.app/

https://shap.readthedocs.io/en/latest/genomic_examples.html

https://github.com/slundberg/shap

TPOT: http://epistasislab.github.io/tpot/

https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html

https://langchain.readthedocs.io/en/latest/index.html


Weeok 07 Shared links;
https://arxiv.org/abs/2104.13478 --> Geometric Deep Learning: Grids, Groups, Graphs, Geodesics, and Gauges

https://www.sciencedirect.com/science/article/abs/pii/0893608092900128 --> Kolmogorov's theorem and multilayer neural networks

https://www.sciencedirect.com/science/article/abs/pii/0893608089900038 --> On the approximate realization of continuous mappings by neural networks

https://www.stat.cmu.edu/~cshalizi/mreg/15/lectures/13/lecture-13.pdf --> Lecture 13: Simple Linear Regression in Matrix Format

https://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=4,2&seed=0.26911&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false --> Tensorflow playground

https://bjlkeng.github.io/posts/probabilistic-interpretation-of-regularization/ --> A Probabilistic Interpretation of Regularization

https://www.tensorflow.org/datasets/keras_example --> Training a neural network on MNIST with Keras

